---
title: "MA8701 Advanced methods in statistical inference and learning"
author: "Mette Langaas IMF/NTNU"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: yes
    toc_float: yes
    code_download: yes
    toc_depth: 3
  ioslides_presentation: default
  beamer_presentation:
    slide_level: 1
    keep_tex: yes
  pdf_document:
    toc: yes
    toc_depth: 2
subtitle: 'L2: Shrinkage - the beginning'
---

```{r setup, include=TRUE,echo=FALSE}
suppressPackageStartupMessages(library(knitr))
knitr::opts_chunk$set(echo = FALSE, message=FALSE,warning = FALSE, error = FALSE)
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(reticulate))
#reticulate::use_python("/usr/bin/python3",required=TRUE)
suppressPackageStartupMessages(library(ggpubr))
suppressPackageStartupMessages(library(cowplot))
suppressPackageStartupMessages(library(magick))
suppressPackageStartupMessages(library(pheatmap))
suppressPackageStartupMessages(library(amap))
```


```{r,eval=FALSE}
#http://zevross.com/blog/2017/06/19/tips-and-tricks-for-working-with-images-and-figures-in-r-markdown-documents/
# options in r chunk settings
# out.width="100%"
# dpi=72

# ny figur? office hours + mer om shrinkage + optimism Cov(yhat_i,y_i)
include_graphics("./overviewv2.png")
```

# Shrinkage

## Why?

* or "Regularized linear and generalized linear models"
* or "Penalized maximum likelihood estimation"
* and also "Sparse models".

Focus is on generalized linear models in this part, but we will also consider shrinkage in the next parts of this course (then for "more complex" method).

**Question:** in linear models (linear regression, generalized linear regression) we mainly work with methods where parameter estimates are unbiased - but might have high variance and not give very good prediction performance. Can we use penalization to produce parameter estimates with some bias but less variance, so that the prediction performance is improved?

---

We will look at different ways of penalization - mainly what is called ridge and lasso methods.

Lasso is a sparse method. In sparse statistical models a _small number of covariates_ play an important role. 

HTW (page 2): _Bet on sparsity principle:
Use a procedure that does well in sparse problems, since no procedure
does well in dense problems._

Shrinkage (penalization, regularization) methods are especially suitable in situations where we have more covariates than observations $N<<p$. Two examples are 

* in medicine with genetic data, where the number of patient samples is less than the number of genetic markers studied,
* in analysis of text (more to come in L3)

## Literature

* [ELS] The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition (Springer Series in Statistics, 2009) by Trevor Hastie, Robert Tibshirani, and Jerome Friedman.

* [HTW] Hastie, Tibshirani, Wainwrigh: "Statistical Learning with Sparsity: The Lasso and Generalizations". CRC press. [Ebook](https://trevorhastie.github.io/).

## Plan

* L2: ELS 3.2, 3.4, HTW Ch2 + some from 5? maybe text?
* L3: HTW Ch 3-4+5, how to analyse text
* L4: Post selection inference (HTW 6 or Berk et al article or other articles)

# Linear models
(ELS 3.2, 3.4, HTW Ch 2)

## Set-up

Random response $Y$ and $p$-dimensional (random) covariates $X$.

Training data: $N$ (independent) observations: $(y_i,x_i)$, where $x_i$ is a column vector with $p$ covariates (features).

## Linear regression model
(ELS 3.2)

Additive noise model
$$ Y=f(X)+\varepsilon$$
with $\text{E}(\varepsilon)=0$ and $\text{Var}(\varepsilon)=\sigma^2_{\varepsilon}$.

With squared loss, we remember that the optimal $f(X)=\text{E}(Y \mid X)$.

Linear regression model - we assumes that
$$f(X)=\beta_0+\sum_{j=1}^p X_{j}\beta_j $$
is linear in $X$, or that is a good approximation.

---

From TMA4267 we know that if $(X,Y)$ is jointly multivariate normal, then the conditional distribution of $Y\mid X$ has mean that is linear in $X$ and variance that is independent of $X$. Brush-up: See classnotes [page 8](https://www.math.ntnu.no/emner/TMA4267/2017v/TMA4267V2017Part2.pdf).

The unknown parameters are the regression coefficients $\beta_0,\ldots,\beta_p$ and the error variance $\sigma^2_{\varepsilon}$.

The covariates $X$ can be both quantitative or qualitative, be made of basis expansions or interactions - and more. For qualitative covariates often a dummy variable coding is used.
Brush-up: See [TMA4315 GLM Module 2](https://www.math.ntnu.no/emner/TMA4315/2018h/2MLR.html#categorical_covariates_-_dummy_and_effect_coding).

## Least squares estimation




The same results are found using likelihood theory, if we assume that $Y\sim N$. See [TMA4315 GLM Module 2](https://www.math.ntnu.no/emner/TMA4315/2018h/2MLR.html#likelihood_theory_(from_b4))

---

$$ y_i=\beta_0+\sum_{j=1}^p x_{ij}\beta_j + \varepsilon_i$$

Exercise (ELS ex 3.3) for proving the Gauss-Markov theorem. 

## Centering covariates and response

# Ridge regression

se slides fra Riccardo L3 for masse fint om ridge
bevis og det hele

# Lasso

Hele L4 fra Riccardo er om lasso

Ex 3 og 4 ogsÃ¥ fra Riccardo


webinar on glmnet from 2019: http://youtu.be/BU2gjoLPfDc

# Software

```{r}
include_graphics("https://glmnet.stanford.edu/reference/figures/logo.png")
```

![https://glmnet.stanford.edu/reference/figures/logo.png]

We will use the `glmnet` implementation:

* [R  glmnet on CRAN](https://cran.r-project.org/web/packages/glmnet/index.html)
with [resources](http://www.stanford.edu/~hastie/glmnet).
   + [Getting started](https://glmnet.stanford.edu/articles/glmnet.html)
   + [GLM with glmnet](https://glmnet.stanford.edu/articles/glmnetFamily.html)
* [Python glmnet](https://web.stanford.edu/~hastie/glmnet_python/)

# Exercises

## Gauss-Markov theorem

The LS is unbiased with the smallest variance 

ELS exercise 3.3

## 
